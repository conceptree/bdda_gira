{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/conceptree/bdda_gira/blob/main/bdda_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Projecto de Base de Dados Distribuidas Avançadas\n",
        "\n",
        "## Autores\n",
        "\n",
        "*   Nuno Rodrigues\n",
        "*   Tiago Alves\n",
        "\n",
        "## Introdução\n",
        "\n",
        "Este projecto visa aplicar as metodologias dadas em aula, referentes a base de dados distribuidas, com recurso aos dados referentes ao aluguer de bicicletas na cidade de Lisboa através do programa [GIRA](https://www.gira-bicicletasdelisboa.pt/) com uma relação às condições meteorológicas presentes durante o periodo em estudo.\n",
        "\n",
        "Neste coLab iremos precorrer os diferentes passos para a tansformação e tratamento dos dados em enface.\n",
        "\n",
        "## Origem dos dados\n",
        "\n",
        "- Bicicletas (GIRA) - https://dados.gov.pt/pt/datasets/gira-bicicletas-de-lisboa-historico/\n",
        "- Meteorológicos - https://www.visualcrossing.com/"
      ],
      "metadata": {
        "id": "x-eEyZhWi2gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 1\n",
        "\n",
        "Este primeiro passo consiste na aplicação do script responsável pelo tratamento dos dados referentes às diferentes estações de bicicletas da cidade de Lisboa.\n",
        "\n",
        "Iremos tratar dois datasets retirados da origem, já mencionada em cima, separados com base no intervalo temporal de **2022-01-01** a **2023-02-16**.\n",
        "\n",
        "Para este tratamento iremos aplicar as seguintes transformações:\n",
        "\n",
        "1.   Criar a coluna ID que será proveniente do primeiro resultado de um split realizado na coluna original \"desigcomercial\".\n",
        "2.   Criar a coluna station que será que será proveniente do segundo resultado de um split realizado na coluna original \"desigcomercial\" e discartando completamente o caracter usado para a divisão (ex: \"-\").\n",
        "3. Renomeamos as colunas originais que pretendemos manter da seguinte forma:\n",
        "  - 'numbicicletas': 'bikes_available',\n",
        "  - 'numdocas': 'total_docks',\n",
        "  - 'estado': 'state'\n",
        "\n",
        "4. Criamos a coluna latitude cuja coordenada é retirada da coluna original \"position\".\n",
        "5. Criamos a coluna longitude cuja coordenada é retirada da coluna original \"position\".\n",
        "6. Criamos a coluna date que retiramos da coluna original \"entity_ts\".\n",
        "7. Criamos a coluna tim que retiramos da coluna original \"entity_ts\" e formatamos como HH:MM:SS.\n",
        "\n",
        "Corremos estas transformações para o dataset 1 e 2 de forma a termos gira_h1_cleaned_dataset.csv e gira_h2_cleaned_dataset.csv finais para o passo seguinte."
      ],
      "metadata": {
        "id": "6Q-E0ROjw7Iw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcnHdGmwiyzQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_dataset.csv' with the path to your actual dataset file\n",
        "file_path = 'Gira_H1.csv'\n",
        "\n",
        "# Read the dataset\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print column names to verify\n",
        "print(\"Columns in the dataset:\", df.columns.tolist())\n",
        "\n",
        "# Ensure that the column names used below match exactly with what's printed above\n",
        "# Splitting 'designcomercial' into 'ID' and 'station'\n",
        "df[['ID', 'station']] = df['desigcomercial'].str.split('-', n=1, expand=True)\n",
        "\n",
        "# Renaming columns\n",
        "df.rename(columns={'numbicicletas': 'bikes_available',\n",
        "                   'numdocas': 'total_docks',\n",
        "                   'estado': 'state'}, inplace=True)\n",
        "\n",
        "# Extracting latitude and longitude from 'position'\n",
        "df['latitude'] = df['position'].apply(lambda x: eval(x)['coordinates'][1])\n",
        "df['longitude'] = df['position'].apply(lambda x: eval(x)['coordinates'][0])\n",
        "\n",
        "# Extracting date and time from 'entity_ts'\n",
        "df['date'] = pd.to_datetime(df['entity_ts']).dt.date\n",
        "df['time'] = pd.to_datetime(df['entity_ts']).dt.strftime('%H:%M:%S')\n",
        "\n",
        "# Selecting the required columns in the specified order\n",
        "df = df[['ID', 'station', 'bikes_available', 'total_docks', 'state', 'latitude', 'longitude', 'date', 'time']]\n",
        "\n",
        "# Write the cleaned data to a new CSV file\n",
        "output_file = 'gira_h1_cleaned_dataset.csv'\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Cleaned dataset saved as '{output_file}'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 2\n",
        "\n",
        "Neste passo corremos aplicamos a concatenação dos datasets resultantes do passo 1 de forma a ficarmos com apenas um dataset referente às bicicletas chamado gira_final.csv."
      ],
      "metadata": {
        "id": "S2-nLMy10nG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "df_01 = pd.read_csv('gira_h1_cleaned_dataset.csv')\n",
        "df_02 = pd.read_csv('gira_h2_cleaned_dataset.csv')\n",
        "\n",
        "# Concatenate the datasets\n",
        "concatenated_df = pd.concat([df_01, df_02])\n",
        "\n",
        "# Optionally, sort by date if the datasets are not already sorted\n",
        "# Replace 'date_column_name' with the name of your date column\n",
        "concatenated_df.sort_values(by='date', inplace=True)\n",
        "\n",
        "# Reset the index of the concatenated dataset\n",
        "concatenated_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Save the concatenated dataset to a new CSV file\n",
        "concatenated_df.to_csv('gira_final.csv', index=True, index_label=\"index\")\n",
        "\n",
        "print(\"Concatenated dataset saved.\")\n"
      ],
      "metadata": {
        "id": "-g1WkVPo04pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 3\n",
        "\n",
        "Agora só nos falta criar as colunas referentes aos indicadores meteorológicos que pretendemos para este estudo, sendo os mesmos:\n",
        "\n",
        "*   Temperatura\n",
        "*   Precipitação\n",
        "*   Temperatura que se faz sentir\n",
        "\n",
        "Este indicadores serão retirados do dataset Lx_Meteo.csv que por sua vez inclui todos os dados meteorológicos da cidade de Lisboa dentro do intervalo de tempo em estudo.\n",
        "\n",
        "Após este passo iremos terminar com o que será o nosso dataset final que vai pelo nome de final_dataset.csv."
      ],
      "metadata": {
        "id": "B1GJofYS1WQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming the concatenated dataset is available at the given path\n",
        "concatenated_path = 'gira_final.csv'  # Update this with the actual file path\n",
        "concatenated_df = pd.read_csv(concatenated_path)\n",
        "\n",
        "# Load the meteorological data\n",
        "meteo_path = './Lx_Meteo.csv'\n",
        "meteo_df = pd.read_csv(meteo_path)\n",
        "\n",
        "# Ensure date formats are aligned (modify if necessary)\n",
        "concatenated_df['date'] = pd.to_datetime(concatenated_df['date'])\n",
        "meteo_df['datetime'] = pd.to_datetime(meteo_df['datetime'])\n",
        "\n",
        "# Merge the datasets on the date column\n",
        "merged_df = pd.merge(concatenated_df, meteo_df[['datetime', 'feelslike', 'temp', 'precip']],\n",
        "                     left_on='date', right_on='datetime', how='left')\n",
        "\n",
        "# Drop the extra 'datetime' column if not needed\n",
        "merged_df.drop('datetime', axis=1, inplace=True)\n",
        "\n",
        "# Save the merged dataset\n",
        "merged_df.to_csv('final_dataset.csv', index=False)\n",
        "\n",
        "print(\"Final merged dataset saved.\")\n"
      ],
      "metadata": {
        "id": "Z6B_Ypjz1i7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusão\n",
        "\n",
        "Após todas as transformações realizadas nos passos em cima mencionados, temos então, o que serão os dados que utilizaremos como base de estudo para este projecto.\n",
        "\n",
        "Dos mesmos podemos então abordar algunas das seguintes análises e áreas negócio:\n",
        "\n",
        "\n",
        "### Análise de Procura:\n",
        "\n",
        "* Impacto do Clima no Uso de Bicicletas: Analisar como diferentes condições climáticas (como temperatura, precipitação e sensação térmica) afetam o uso de bicicletas. Isso pode ajudar a prever a procura em diferentes condições climáticas e diferentes pontos da cidade.\n",
        "\n",
        "* Tendências Sazonais: Examinar padrões de uso em diferentes estações do ano para identificar períodos de pico e baixa procura.\n",
        "\n",
        "### Eficiência Operacional:\n",
        "\n",
        "* Necessidades de Reequilíbrio: Identificar estações que frequentemente ficam sem bicicleta e correlacionar isso com padrões climáticos para otimizar a redistribuição de bicicletas.\n",
        "\n",
        "* Agendamento de Manutenção: Procurar padrões no uso de bicicletas para programar manutenções em momentos de menor demanda, possivelmente influenciados pelas condições climáticas. Isto pode ser feito com base no estado actual de cada estação.\n",
        "\n",
        "### Análise do Comportamento do Cliente:\n",
        "\n",
        "* Duração e Distância: Entender como as condições climáticas afetam a duração e a distância das viagens de bicicleta.\n",
        "\n",
        "* Horários de Pico de Uso: Determinar os horários do dia em que as bicicletas são mais requisitadas em cada uma das estações e como isso pode variar com o clima.\n",
        "\n",
        "### Estratégias de Marketing e Promoção:\n",
        "\n",
        "* Promoções Direcionadas: Desenvolver promoções baseadas no clima, como descontos em dias ensolarados ou durante condições climáticas menos populares para aumentar o uso.\n",
        "\n",
        "### Análise de Segurança:\n",
        "\n",
        "* Incidentes Relacionados ao Clima: Analisar se certas condições climáticas levam a mais acidentes ou problemas de segurança.\n",
        "\n",
        "* Visibilidade e Condições: Avaliar o impacto da visibilidade (chuva forte) na segurança do uso de bicicletas. (Aqui apesar de interessante, talvez seja necessário cruzar dados de incidentes com bicicletas)\n",
        "\n",
        "### Impacto Ambiental:\n",
        "\n",
        "* Reduções de Emissões: Estimar a redução nas emissões de carbono devido ao uso de bicicletas em vez de dirigir, possivelmente influenciado pelas condições climáticas. (Algo também interessante mas, novo cruzamento seria necessário com dados referentes às emissões em determinados periodos)\n",
        "\n",
        "* Promoção do Transporte Verde: Usar os dados para promover o ciclismo como um modo de transporte sustentável, especialmente em dias claros e com baixa poluição. (Argumento provindo do ponto anterior)\n",
        "\n",
        "### Análise Geográfica:\n",
        "\n",
        "* Rotas e Áreas Populares: Identificar as rotas e áreas mais populares para andar de bicicleta e como as condições climáticas afetam essas escolhas.\n",
        "\n",
        "* Oportunidades de Expansão: Usar os dados para identificar locais potenciais\n",
        "para novas estações de bicicletas.\n",
        "\n",
        "### Análise Financeira:\n",
        "\n",
        "* Flutuações de Receita: Investigar como as condições climáticas impactam a receita do uso de bicicletas.\n",
        "\n",
        "### Previsão:\n",
        "\n",
        "* Modelação Preventiva: Desenvolver modelos preditivos para prever a procura de compartilhamento de bicicletas com base nas previsões meteorológicas.\n",
        "\n",
        "* Modelos de Preços Dinâmicos: Explorar estratégias de preços dinâmicos baseadas na demanda antecipada influenciada pelas condições climáticas."
      ],
      "metadata": {
        "id": "RMdNfuT62kXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HDFS e Distribuição\n",
        "\n",
        "Parte importante deste projecto é agora aproveitar os dados que temos e colocar em prática a metodologias de base de dados distribuidas dadas em aula com recurso ao ecosistema Hadoop.\n",
        "\n",
        "Para os passos seguintes recorremos ao stack Hadoop presente em https://github.com/fabiogjardim/bigdata_docker\n",
        "\n",
        "Neste projecto vamos abordar este requisto da seguinte maneira:\n",
        "\n",
        "1. Vamos partir os dados em três csv´s diferentes\n",
        "2. Vamos colocar os dados no HDFS através dos comandos:\n",
        "\n",
        "\n",
        "Aceder ao ambiente de bash do contentor do HDFS\n",
        "\n",
        "```\n",
        " docker exec -it datanode bash\n",
        "```\n",
        "E executar os comandos:\n",
        "```\n",
        "hdfs dfs -mkdir /gira/\n",
        "hdfs dfs -put /caminho/local/dos/dados /caminho/no/hdfs\n",
        "```\n",
        "\n",
        "> **_NOTA:_** O segundo comando será apenas corrido nos passos mais a frente assim que se souber o caminho da pasta local partilhada com o contentor em questão sendo que o /caminho/no/hdfs será então o já criado /gira/\n",
        "\n",
        "Vamos primeiro dividir o nosso dataset em três partes iguais recorrrendo ao seguinte script:\n"
      ],
      "metadata": {
        "id": "lhPa641LTaiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def split_csv(input_file, output_files):\n",
        "    # Read the input CSV file\n",
        "    data = pd.read_csv(input_file)\n",
        "\n",
        "    # Calculate the size of each split\n",
        "    split_size = len(data) // len(output_files)\n",
        "\n",
        "    # Split the data and save to new files\n",
        "    for i, output_file in enumerate(output_files):\n",
        "        if i < len(output_files) - 1:\n",
        "            data_split = data.iloc[i*split_size:(i+1)*split_size]\n",
        "        else:\n",
        "            # Ensure the last file includes any remaining rows\n",
        "            data_split = data.iloc[i*split_size:]\n",
        "        data_split.to_csv(output_file, index=False)\n",
        "\n",
        "# Usage\n",
        "input_csv = 'final_dataset.csv'\n",
        "output_csvs = ['data_1.csv', 'data_2.csv', 'data_3.csv']\n",
        "split_csv(input_csv, output_csvs)"
      ],
      "metadata": {
        "id": "c1yylyC-Utha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uma vez que tenhamos os nossos dados separados temos então de partilhar a sua localização na máquina local com um volume partilhado no contentor docker que temos a correr.\n",
        "\n",
        "Para saber qual o nosso directório local onde teremos de colocar os dados de forma a serem acessíveis no contentor, temos de verificar qual o volume partilhado do mesmo, que neste caso será onde se encontra a correr o HDFS que segundo a documentação da docker que estamos a utilizar corre sobe o nome de \"datanode\".\n",
        "\n",
        "Para obter essa informação precisamos apenas de correr:\n",
        "\n",
        "\n",
        "```\n",
        "docker inspect datanode\n",
        "```\n",
        "\n",
        "O comando em cima deverá imprimir a informação de \"Mounts\" onde concretamente só necessitamos das seguintes informações:\n",
        "\n",
        "\n",
        "```\n",
        "\"Mounts\": [\n",
        "    {\n",
        "        \"Type\": \"bind\",\n",
        "        \"Source\": \"/caminho/na/sua/maquina\",\n",
        "        \"Destination\": \"/caminho/no/container\",\n",
        "        ...\n",
        "    }\n",
        "]\n",
        "```\n",
        "\n",
        "Agora temos tudo o que necessitamos para partilhar os dados transformados com o contentor do HDFS basta então copiar os mesmos para a pasta referida na \"Source\" e proceder aos seguintes comandos:\n",
        "\n",
        "\n",
        "```\n",
        "docker exec -it datanode /bin/bash\n",
        "```\n",
        "\n",
        "Para aceder ao ambiente bash do contentor em questão:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "hdfs dfs -put /hadoop/dfs/data/* /gira/\n",
        "```\n",
        "\n",
        "Relembrando que nos primeiros passos em cima \"gira\" foi o nome que estabelecemos enquanto directório no HDFS.\n",
        "\n",
        "Uma vez todos os passos em cima estejam realizados com sucesso podemos então comprovar a distribuição através do seguinte comando:\n",
        "\n",
        "```\n",
        "hdfs dfs -ls /gira/\n",
        "```\n",
        "\n",
        "Que deverá imprimir algo como:\n",
        "\n",
        "```\n",
        "-rw-r--r--   3 root supergroup  146754628 2023-12-29 16:33 /gira/data_1.csv\n",
        "-rw-r--r--   3 root supergroup  148435940 2023-12-29 16:34 /gira/data_2.csv\n",
        "-rw-r--r--   3 root supergroup  142260097 2023-12-29 16:34 /gira/data_3.csv\n",
        "```\n",
        "\n",
        "E para verificar a localização dos blocos de dados nos diferentes nós do cluster:\n",
        "\n",
        "```\n",
        "hdfs fsck /gira/ -files -blocks\n",
        "```\n",
        "\n",
        "```\n",
        "/// OUTROS BLOCOS LISTADOS\n",
        "\n",
        "/gira/data_1.csv 146754628 bytes, 2 block(s):  Under replicated BP-810256244-172.18.0.7-1702412100106:blk_1073742083_1262. Target Replicas is 3 but found 1 replica(s).\n",
        " Under replicated BP-810256244-172.18.0.7-1702412100106:blk_1073742084_1263. Target Replicas is 3 but found 1 replica(s).\n",
        "0. BP-810256244-172.18.0.7-1702412100106:blk_1073742083_1262 len=134217728 repl=1\n",
        "1. BP-810256244-172.18.0.7-1702412100106:blk_1073742084_1263 len=12536900 repl=1\n",
        "\n",
        "/gira/data_2.csv 148435940 bytes, 2 block(s):  Under replicated BP-810256244-172.18.0.7-1702412100106:blk_1073742085_1264. Target Replicas is 3 but found 1 replica(s).\n",
        " Under replicated BP-810256244-172.18.0.7-1702412100106:blk_1073742086_1265. Target Replicas is 3 but found 1 replica(s).\n",
        "0. BP-810256244-172.18.0.7-1702412100106:blk_1073742085_1264 len=134217728 repl=1\n",
        "1. BP-810256244-172.18.0.7-1702412100106:blk_1073742086_1265 len=14218212 repl=1\n",
        "\n",
        "/gira/data_3.csv 142260097 bytes, 2 block(s):  Under replicated BP-810256244-172.18.0.7-1702412100106:blk_1073742087_1266. Target Replicas is 3 but found 1 replica(s).\n",
        " Under replicated BP-810256244-172.18.0.7-1702412100106:blk_1073742088_1267. Target Replicas is 3 but found 1 replica(s).\n",
        "0. BP-810256244-172.18.0.7-1702412100106:blk_1073742087_1266 len=134217728 repl=1\n",
        "1. BP-810256244-172.18.0.7-1702412100106:blk_1073742088_1267 len=8042369 repl=1\n",
        "\n",
        "/gira/in_use.lock 12 bytes, 1 block(s):  Under replicated BP-810256244-172.18.0.7-1702412100106:blk_1073742089_1268. Target Replicas is 3 but found 1 replica(s).\n",
        "0. BP-810256244-172.18.0.7-1702412100106:blk_1073742089_1268 len=12 repl=1\n",
        "\n",
        "Status: HEALTHY\n",
        " Total size:\t851835484 B\n",
        " Total dirs:\t9\n",
        " Total files:\t181\n",
        " Total symlinks:\t\t0\n",
        " Total blocks (validated):\t184 (avg. block size 4629540 B)\n",
        " Minimally replicated blocks:\t184 (100.0 %)\n",
        " Over-replicated blocks:\t0 (0.0 %)\n",
        " Under-replicated blocks:\t184 (100.0 %)\n",
        " Mis-replicated blocks:\t\t0 (0.0 %)\n",
        " Default replication factor:\t3\n",
        " Average block replication:\t1.0\n",
        " Corrupt blocks:\t\t0\n",
        " Missing replicas:\t\t368 (66.666664 %)\n",
        " Number of data-nodes:\t\t1\n",
        " Number of racks:\t\t1\n",
        "FSCK ended at Fri Dec 29 16:38:42 UTC 2023 in 13 milliseconds\n",
        "\n",
        "\n",
        "The filesystem under path '/gira' is HEALTHY\n",
        "```\n",
        "\n",
        "Neste print conseguimos comprovar a replicação dos nossos dados ainda que a com uma réplica apenas. Isto deve-se ao facto de estarmos apenas a correr esta mesma docker em apenas um Node para efeitos de demonstração.\n",
        "\n",
        "Uma vez em ambiente de produção teriamos vários Data Nodes distribuídos por diferentes máquinas físicas ou virtuais, permitindo que o sistema mantenha várias cópias dos dados em diferentes nós para tolerância a falhas. No entanto, em um único nó, a alta disponibilidade e a tolerância a falhas não são possíveis, pois se o único nó falhar, todos os dados ficaram inacessíveis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "btnsDlVQVJEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consultas via Hive\n",
        "\n",
        "Agora falta apenas utilizarmos uma das ferramentas do stack para a criação de tabelas através dos dados inseridos no HDFS e efectuar consultas aos mesmos.\n",
        "\n",
        "Para isto vamos utilizar o Hive através do interface web Hue.\n",
        "\n",
        "Primeiro temos de criar a nossa tabela à qual vamos efectuar as consultas necessárioas. Para isso vamos então proceder aos seguintes passos:\n",
        "\n",
        "1. Aceder ao Hue que corre em http://localhost:8888/hue/home/\n",
        "2. Escolher o Hive através do menu de \"Query\" e em seguida \"Editor\" e depois Hive.\n",
        "3. Tendo em conta que o Hive conta só encontrar ficheiros de texto nos diretórios onde devem estar os dados presentes, teremos de mover os nossos datasets para uma pasta onde isso seja garantido. Para isso só precisamos correr:\n",
        "\n",
        "```\n",
        "hdfs dfs -mkdir /gira/hive_query/\n",
        "hdfs dfs -cp /gira/data_*.csv /gira/hive_query/\n",
        "```\n",
        "\n",
        "4. Criar a tabela através da seguinte query:\n",
        "```\n",
        "CREATE EXTERNAL TABLE bikes_data (\n",
        "    `index` INT,\n",
        "    ID INT,\n",
        "    station STRING,\n",
        "    bikes_available INT,\n",
        "    total_docks INT,\n",
        "    state STRING,\n",
        "    latitude DOUBLE,\n",
        "    longitude DOUBLE,\n",
        "    `date` STRING,\n",
        "    time STRING,\n",
        "    feelslike DOUBLE,\n",
        "    temp DOUBLE,\n",
        "    precip DOUBLE\n",
        ")\n",
        "ROW FORMAT DELIMITED\n",
        "FIELDS TERMINATED BY ','\n",
        "STORED AS TEXTFILE\n",
        "LOCATION 'hdfs://namenode:8020/gira/hive_query/'\n",
        "TBLPROPERTIES (\"skip.header.line.count\"=\"1\");\n",
        "```\n",
        "\n",
        "5. Proceder a uma consulta simples como:\n",
        "\n",
        "```\n",
        "SELECT * FROM bikes_data\n",
        "```\n",
        "\n",
        "E devemos conseguir então visualizar todas as entradas dos nossos dados."
      ],
      "metadata": {
        "id": "MUeCPiifl4sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusão\n",
        "\n",
        "Com todos os passos em cima podemos então validar os seguintes pontos:\n",
        "\n",
        "## Configuração do Ambiente Hadoop\n",
        "\n",
        "Foi estabelecido um ambiente Hadoop com um único nó, utilizando Docker. Este ambiente é adequado para desenvolvimento e testes, mas não recomendado para produção devido à incapacidade de garantia da integridade dos dados com apenas um nó.\n",
        "\n",
        "## Verificação de Dados no HDFS\n",
        "\n",
        "Usando o comando hdfs fsck, foi possível verificar a integridade dos arquivos no Hadoop Distributed File System (HDFS), garantindo que os dados estavam acessíveis e íntegros.\n",
        "\n",
        "## Integração com o Hue\n",
        "\n",
        "O Hue foi utilizado para facilitar a interação com o HDFS e o Hive. O Hue oferece uma interface gráfica que simplifica a execução de queries e a gestão de dados.\n",
        "\n",
        "## Criação de Tabelas no Hive\n",
        "\n",
        "Foram criadas tabelas no Hive para representar os conjuntos de dados presentes no HDFS. Isso permitiu a execução de queries SQL sobre os dados, facilitando análises e consultas.\n",
        "\n",
        "## Resolução de Problemas com Localização de Arquivos\n",
        "\n",
        "Inicialmente, existiram dificuldades na criação de tabelas devido à especificação incorreta da localização dos arquivos CSV no HDFS devido ao facto do Hive esperar apenas que existam ficheiros de texto nos diretórios especificados algo que não é garantido devido à criação de pastas e ficheiros pelo próprio HDFS. Após ajustes, as tabelas foram criadas com sucesso.\n",
        "\n",
        "A estrutura de diretórios do HDFS foi modificada para organizar os arquivos CSV de forma mais eficiente, facilitando a criação de tabelas no Hive sem colocar em causa a lógica de replicação do HDFS.\n",
        "\n",
        "## Consulta de Dados via Hive\n",
        "\n",
        "Com as tabelas criadas, foi possível realizar consultas SQL sobre os dados dos CSVs. Isso demonstrou a integração eficaz entre o Hive e o HDFS, permitindo análises complexas de dados.\n",
        "\n",
        "## Replicação e Gerenciamento de Dados no HDFS\n",
        "\n",
        "Foi discutida a lógica de replicação do HDFS e como ela se aplica à cópia de arquivos dentro do sistema. Isso é crucial para entender o gerenciamento de espaço e a integridade dos dados no HDFS.\n",
        "\n",
        "## Resultados Alcançados\n",
        "\n",
        "É agora possível realizar análises complexas e consultas em larga escala nos dados armazenados no HDFS usando o Hive.\n",
        "\n",
        "## Flexibilidade e Facilidade de Uso\n",
        "\n",
        "A utilização do Hue para interagir com o Hadoop e o Hive trouxe uma camada adicional de facilidade e flexibilidade para o gerenciamento de dados.\n",
        "\n",
        "## Compreensão da Gestão de Dados no HDFS\n",
        "\n",
        "Os passos realizados proporcionaram uma compreensão prática de como gerenciar e organizar dados dentro de um ambiente Hadoop.\n",
        "\n",
        "### Resumo:\n",
        "\n",
        "Este processo demonstrou como configurar e utilizar um ambiente Hadoop com Docker, realizar a integração com ferramentas como o Hue e o Hive, e como gerenciar dados dentro do HDFS. Além disso, destacou a importância de entender a estrutura de diretórios e a replicação no HDFS para a gestão eficaz de grandes volumes de dados."
      ],
      "metadata": {
        "id": "InS48VFs0_mg"
      }
    }
  ]
}